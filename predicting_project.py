# -*- coding: utf-8 -*-
"""PREDICTING PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18E_p7AmafSYfUrQvGErT9Vko1SPzAZ7O
"""

import pandas as pd
import numpy as np

data = pd.read_csv('PREDICTING PRODUCTION  DATA.csv')

data

data.isna().sum()

data.shape

data.duplicated().sum()

data.info()

data.describe()

data.nunique()

data.columns

data.dtypes

data.head()

data.tail()

data.query("production == '='")

# to drop the rows in the columns having '=' value
data = data[data["production"] != '=']
data = data.dropna(axis=1)

data.shape

data.nunique()
#
# import matplotlib.pyplot as plt
# import seaborn as sns
# data.boxplot(figsize=(15,10))
# plt.show()

"""****EDA (Exploratory Data Analysis) ****"""

# import matplotlib.pyplot as plt
# import seaborn as sns

# scatter plot for Humidity vs Temprature(to get the insights of how they correlated)

# plt.figure(figsize=(10,6))
# sns.scatterplot(x='temperature',y='humidity',data=data)
# plt.show()
#
# plt.figure(figsize=(10,6))
# sns.scatterplot(x='precipitation',y='wind_speed',data=data)
# plt.show()
#
# # Create subplots for better visualization
# fig, axes = plt.subplots(1, 3, figsize=(15, 5))
#
# # N vs Temperature
# sns.scatterplot(x='temperature', y='N', data=data, ax=axes[0])
# axes[0].set_title('N vs Temperature')
#
# # P vs Temperature
# sns.scatterplot(x='temperature', y='P', data=data, ax=axes[1])
# axes[1].set_title('P vs Temperature')
#
# # K vs Temperature
# sns.scatterplot(x='temperature', y='K', data=data, ax=axes[2])
# axes[2].set_title('K vs Temperature')
#
# plt.tight_layout()  # Adjust layout for better spacing
# plt.show()
#
# # to visualize the distribution of season
# plt.figure(figsize=(10, 6))
# plt.rcParams['axes.facecolor'] = 'white'
# sns.countplot(x='season_names', data=data)
# plt.title('Distribution of Seasons')
# plt.xlabel('Season')
# plt.ylabel('Count')
# plt.xticks(rotation=45)
# plt.show()
#
# #histogram for the distribution of crops during the seasons
#
# plt.figure(figsize=(12, 6))
# sns.countplot(x='season_names', hue='crop_names', data=data)
# plt.title('Distribution of Crops Across Seasons')
# plt.xlabel('Season')
# plt.ylabel('Count')
# plt.xticks(rotation=45)  # Rotate x-axis labels if needed
# plt.legend(title='Crop')
# plt.show()
#
# # ploting the histogram based on the seasons individualy
# for season in data['season_names'].unique():
#     plt.figure()
#     sns.countplot(x='crop_names', data=data[data['season_names'] == season])
#     plt.title(f'Distribution of Crops in {season}')
#     plt.xlabel('Crop')
#     plt.ylabel('Count')
#     plt.xticks(rotation=90,fontsize=6)  # Rotate if needed
#     plt.show()
#
# sns.displot(data['area'],kde=True)
# plt.title(f'Distribution of area')
# plt.show()
#
# #distribution of data
#
# #for calling the numerical columns in the dataset
# numerical_cols = data.select_dtypes(include=np.number).columns
#
# for col in numerical_cols:
#   sns.displot(data[col],kde=True, fill=True)
#   plt.title(f'Distribution of {col}')
#   plt.show()
#
# fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10,8))
# axes = axes.flatten()
#
# numerical_cols = data.select_dtypes(include=np.number).columns
#
# for i, col in enumerate(numerical_cols):
#     sns.histplot(data[col], kde=True, fill=True, ax=axes[i])
#     axes[i].set_title(f'Distribution of {col}')
#
# plt.tight_layout()
# plt.show()

data2 = data.copy()

data2.shape

"""***ENCODING THE DATA (One-Hot-Encoder)***"""

from sklearn.preprocessing import OneHotEncoder

encoded_df = pd.get_dummies(data2, columns=['season_names', 'soil_type'],dtype=int)

encoded_df

"""***label Encoding***"""

from sklearn.preprocessing import LabelEncoder

# Assuming `df` is your DataFrame and `category_column` is the column to encode
label_encoder = LabelEncoder()

# Fit and transform the column
encoded_df['crops_name_encoded'] = label_encoder.fit_transform(encoded_df['crop_names'])

Data = encoded_df.drop('crop_names', axis = 1)

Data

Data.isna().sum()

Data.shape

"""***Saciling***

**MinMaxscaler & StandardScaler**
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

standard_columns = ['precipitation', 'wind_speed', 'temperature','area']
minimax_columns = ['N', 'P', 'K', 'humidity']

# Initialize the scalers
standard_scaler = StandardScaler()
minimax_scaler = MinMaxScaler()

# Apply StandardScaler to the selected columns
Data[standard_columns] = standard_scaler.fit_transform(Data[standard_columns])

# Apply MinMaxScaler to the selected columns
Data[minimax_columns] = minimax_scaler.fit_transform(Data[minimax_columns])

Data

Data.isna().sum()

"""***Modeling***"""

from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# Assuming 'production' is the target variable

X = Data.drop('production', axis=1)
y = Data['production']

# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#
# """***1. Lasso Model Regression***"""
#
# lasso_model = Lasso(alpha=1.0)
#
# lasso_model.fit(X_train, y_train)
#
# y_pred = lasso_model.predict(X_test)
# y_pred
#
# from sklearn.metrics import mean_absolute_error
#
# R2 = r2_score(y_test, y_pred)
# MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
# print("R2 Score: ", R2)
# print("Mean Absolute Error: ", MAE)
#
# """***2.SVR (Support Vector Regression)***"""
#
# from sklearn.svm import SVR
#
# svr = SVR()
# svr.fit(X_train,y_train)
#
# y_pred = svr.predict(X_test)
# y_pred
#
# R2 = r2_score(y_test, y_pred)
# MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
# print("R2 Score: ", R2)
# print("Mean Absolute Error: ", MAE)
#
# """***3.RandomForestRegressor***"""
#
# from sklearn.ensemble import RandomForestRegressor
#
# rfr = RandomForestRegressor()
# rfr.fit(X_train,y_train)
#
# y_pred = rfr.predict(X_test)
# y_pred
#
# R2 = r2_score(y_test, y_pred)
# MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
# print("R2 Score: ", R2)
# print("Mean Absolute Error: ", MAE)
#
# """***4.LinearRegression ***"""
#
# from sklearn.linear_model import LinearRegression
#
# lr = LinearRegression()
# lr.fit(X_train, y_train)
#
# y_pred = lr.predict(X_test)
# y_pred
#
# R2 = r2_score(y_test, y_pred)
# MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
# print("R2 Score: ", R2)
# print("Mean Absolute Error: ", MAE)
#
# """***5.DecisionTreeRegressor***"""
#
# from sklearn.tree import DecisionTreeRegressor
#
# model = DecisionTreeRegressor(random_state=42)
#
# # Fit the model on the training data
# model.fit(X_train, y_train)
#
# # Predict on the test data
# y_pred = model.predict(X_test)
# y_pred
#
# R2 = r2_score(y_test, y_pred)
# MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
# print("R2 Score: ", R2)
# print("Mean Absolute Error: ", MAE)

"""***6.GradientBoostingRegressor***"""

from sklearn.ensemble import GradientBoostingRegressor # Import Gradient Boosting Regressor

model = GradientBoostingRegressor() # Create a Gradient Boosting model
model.fit(X_train, y_train) # Fit the model to the training data

# Make predictions
y_pred = model.predict(X_test)
y_pred

R2 = r2_score(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
print("R2 Score: ", R2)
print("Mean Absolute Error: ", MAE)

# """***7.Ridge regression***"""
#
# from sklearn.linear_model import Ridge
#
# # Create a Ridge regression model with a regularization parameter (alpha)
# model = Ridge(alpha=1.0)
#
# # Fit the model to the data
# model.fit(X_train, y_train)
#
# y_pred = model.predict(X_test)
# y_pred
#
# R2 = r2_score(y_test, y_pred)
# MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
# print("R2 Score: ", R2)
# print("Mean Absolute Error: ", MAE)

"""***(Hyperparameter tuning) GridSearch_cv***"""

# from sklearn.model_selection import GridSearchCV
#
#
# max_depth = [10, 20]
# max_features = ['log2', 'sqrt']
# n_estimators = [100, 200]
#
# param_grid = {
#               'max_depth': max_depth,
#               'max_features': max_features,
#               'n_estimators': n_estimators
#              }
# print(param_grid)

# rf_model = GradientBoostingRegressor()
#
# rf_model = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)
#
# rf_model.fit(X_train,y_train)
#
# rf_model.best_score_
#
# rf_model.best_estimator_
#
# rf_model.best_params_

best_model = GradientBoostingRegressor(max_depth=10, max_features='log2', n_estimators=100)
best_model.fit(X_train, y_train)

import pickle
with open('model.pkl', 'wb') as file:
  pickle.dump(best_model, file)